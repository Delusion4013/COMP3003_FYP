{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import BatchNormalization,Flatten,\\\n",
    "Add,Input,Dense, Dropout, Activation, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def import_data():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_construction():\n",
    "    model = models.Sequential(name=\"test\")\n",
    "\n",
    "    inputs = layers.Input(shape=(10,10,10,10))\n",
    "\n",
    "    model = MaxPooling3D(pool_size=(2,2,2))(inputs)\n",
    "    model = Conv3D(32, (2, 2, 2), strides=(1, 1, 1), activation='relu')(model)\n",
    "    model = MaxPooling3D(pool_size=(2,2,2))(model)\n",
    "    model = Conv3D(64, (2, 2, 2), strides=(1, 1, 1), activation='relu')(model)\n",
    "    model = MaxPooling3D(pool_size=(2,2,2))(model)\n",
    "    model = Conv3D(128, (2, 2, 2), strides=(1, 1, 1), activation='relu')(model)\n",
    "    \n",
    "    model = Flatten()(model)\n",
    "    model = Dense(15,activation='relu')(model)\n",
    "    output = model\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.add(MaxPooling3D(pool_size=(2,2), strides=2, padding='same'))\n",
    "    model.add(Conv3D(32, (2,2,2),strides=(1, 1, 1), avtivation='relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "    model.add(Conv3D(64, (2,2,2),strides=(1, 1, 1), avtivation='relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "    model.add(Conv3D(128, (2,2,2),strides=(1, 1, 1), avtivation='relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "    \n",
    "    model.add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peking U example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peking U example\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv3D, BatchNormalization,Activation, MaxPool3D, Dropout, Flatten,Dense\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.rand(100,3,3,3,2)\n",
    "y_train = np.random.rand(100,1)\n",
    "\n",
    "x_test = np.random.rand(3,3,3,2)\n",
    "y_test = np.random.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(Model):\n",
    "    def __init__(self):\n",
    "        super(Baseline, self).__init__()\n",
    "        self.c1 = Conv3D(filters=3, kernel_size=2, padding='same')\n",
    "        self.b1 = BatchNormalization()\n",
    "        self.a1 = Activation('relu')\n",
    "        self.p1 = MaxPool3D(pool_size=2)\n",
    "        self.d1 = Dropout(0.2)\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.f1 = Dense(128, activation='relu')\n",
    "        self.d2 = Dropout(0.2)\n",
    "        self.f2 = Dense(1, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.b1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.p1(x)\n",
    "        x = self.d1(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.d2(x)\n",
    "        y = self.f2(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Baseline()\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.MeanAbsoluteError,metrics=['MeanAbsoluteError'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train,epochs=5,validation_data=(x_test,y_test), validation_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Official Example\n",
    "[Link](https://www.tensorflow.org/tutorials/images/cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "depth = 109\n",
    "height = 104\n",
    "width = 85\n",
    "input_channels = 2\n",
    "frame_count = 10\n",
    "input_shape =(batch_size, depth,height,width,input_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.rand(100,depth,height,width,input_channels)\n",
    "y_train = np.random.rand(100,1)\n",
    "\n",
    "x_test = np.random.rand(20,depth,height,width,input_channels)\n",
    "y_test = np.random.rand(20,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 107, 102, 83, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test the Conv3D layer\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.Conv3D(filters=1, kernel_size=3, activation='relu', input_shape=input_shape[1:])(x)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 54, 52, 42, 2)\n"
     ]
    }
   ],
   "source": [
    "# Test the MaxPooling layer\n",
    "inputs = tf.keras.Input(shape=input_shape[1:])\n",
    "layer = tf.keras.layers.MaxPooling3D(pool_size=2)\n",
    "outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv3D(filters=2, kernel_size=3, activation='relu', input_shape=input_shape[1:]))\n",
    "model.add(layers.MaxPooling3D(pool_size=2))\n",
    "model.add(layers.Conv3D(filters=4, kernel_size=3, activation='relu'))\n",
    "model.add(layers.MaxPooling3D(pool_size=2))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(2, activation='relu'))\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_3 (Conv3D)            (None, 107, 102, 83, 2)   110       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 53, 51, 41, 2)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 51, 49, 39, 4)     220       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 25, 24, 19, 4)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 45600)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 91202     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                30        \n",
      "=================================================================\n",
      "Total params: 91,562\n",
      "Trainable params: 91,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 42s 423ms/sample - loss: 2.2845 - accuracy: 0.0000e+00 - val_loss: 1.0138 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 36s 358ms/sample - loss: 0.6580 - accuracy: 0.0000e+00 - val_loss: 0.1018 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 30s 297ms/sample - loss: 0.0533 - accuracy: 0.0000e+00 - val_loss: 0.0028 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 29s 292ms/sample - loss: 0.0013 - accuracy: 0.0000e+00 - val_loss: 4.5245e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 29s 287ms/sample - loss: 2.1515e-05 - accuracy: 0.0000e+00 - val_loss: 8.3446e-07 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 35s 347ms/sample - loss: 3.9577e-07 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 30s 303ms/sample - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 27s 270ms/sample - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 43s 430ms/sample - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 38s 378ms/sample - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, \n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite the agrd input into txt format\n",
    "\n",
    "fin = open(\"data/2yel/_fld-01_obj-01.agrd.2yel\", \"rt\")\n",
    "fout = open(\"out.txt\", \"wt\")\n",
    "\n",
    "\n",
    "for line in fin:\n",
    "    fout.write(' '.join(line.split()))\n",
    "    fout.write(\"\\n\")\n",
    "\n",
    "fin.close()\n",
    "fout.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the txt input and reshape it\n",
    "import numpy as np\n",
    "npdata = np.loadtxt('out.txt',delimiter=\" \")\n",
    "\n",
    "# data = npdata.reshape(109,104,85)\n",
    "Fdata = npdata.reshape((109,104,85),order='F') # Using order F to maintain x-y-z order\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_frame(path):\n",
    "    fin = open(path, \"rt\")\n",
    "    fout = open(\"out.txt\", \"wt\")\n",
    "\n",
    "    for line in fin:\n",
    "        fout.write(' '.join(line.split()))\n",
    "        fout.write(\"\\n\")\n",
    "\n",
    "    fin.close()\n",
    "    fout.close()\n",
    "\n",
    "    npdata = np.loadtxt('out.txt',delimiter=\" \")\n",
    "    frame_data = npdata.reshape((109,104,85),order='F') # Using order F to maintain x-y-z order\n",
    "    return frame_data\n",
    "\n",
    "path = \"data/2yel/_fld-01_obj-01.agrd.2yel\"\n",
    "test = read_single_frame(path)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_single_frame(path)\n",
    "fout = open(\"data/b2yel/_fld-01_obj-02.npy\", \"wb\")\n",
    "\n",
    "np.save(fout,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_single_frame(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/b2yel/_fld-01_obj-01.npy\", \"rb\") as f:\n",
    "    firstFrame = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traverse the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/2yel/_fld-01_obj-01.agrd.2yel\n",
      "data/2yel/_fld-01_obj-02.agrd.2yel\n",
      "data/2yel/_fld-01_obj-03.agrd.2yel\n",
      "data/2yel/_fld-01_obj-04.agrd.2yel\n",
      "data/2yel/_fld-01_obj-05.agrd.2yel\n",
      "data/2yel/_fld-01_obj-06.agrd.2yel\n",
      "data/2yel/_fld-01_obj-07.agrd.2yel\n",
      "data/2yel/_fld-01_obj-08.agrd.2yel\n",
      "data/2yel/_fld-01_obj-09.agrd.2yel\n",
      "data/2yel/_fld-01_obj-10.agrd.2yel\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"data/2yel/\"\n",
    "files =os.listdir(path) #采用listdir来读取所有文件\n",
    "files.sort() #排序\n",
    "for file_ in files:     #循环读取每个文件名\n",
    "#    print(path +file_)\n",
    "    if not os.path.isdir(path +file_):  #判断该文件是否是一个文件夹\n",
    "        f_name = str(file_)\n",
    "        print(path + f_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_trajactory(path, input_shape):\n",
    "    frames = []\n",
    "    files =os.listdir(path) #采用listdir来读取所有文件\n",
    "    files.sort() #排序\n",
    "    for file_ in files:     #循环读取每个文件名\n",
    "        # if not os.path.isdir(path +file_):  #判断该文件是否是一个文件夹\n",
    "        f_name = str(file_)\n",
    "        frames.append(read_single_frame(path+f_name))\n",
    "        \n",
    "    frames = np.stack((frames), axis=3)\n",
    "    return frames\n",
    "\n",
    "path = \"data/2yel/\"\n",
    "testFrames = traverse_trajactory(path,input_shape)\n",
    "testFrames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/b2yel/_fld-01_obj-01.npy\n",
      "_fld-01_obj-02.agrd.2yel\n",
      "_fld-01_obj-03.agrd.2yel\n",
      "_fld-01_obj-04.agrd.2yel\n",
      "_fld-01_obj-05.agrd.2yel\n",
      "_fld-01_obj-06.agrd.2yel\n",
      "_fld-01_obj-07.agrd.2yel\n",
      "_fld-01_obj-08.agrd.2yel\n",
      "_fld-01_obj-09.agrd.2yel\n",
      "_fld-01_obj-10.agrd.2yel\n"
     ]
    }
   ],
   "source": [
    "path = \"data/2yel/\"\n",
    "bpath = \"data/b2yel/\"\n",
    "files =os.listdir(path) #采用listdir来读取所有文件\n",
    "files.sort() #排序\n",
    "for file_ in files:     #循环读取每个文件名\n",
    "    f_name = str(file_)\n",
    "    if os.path.exists(bpath+f_name.split(\".\")[0]+\".npy\"):\n",
    "        print(bpath+f_name.split(\".\")[0]+\".npy\")\n",
    "    else:\n",
    "        print(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109, 104, 85, 10)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic version of stacking input frames at last axis\n",
    "def traverse_trajactory(path, bpath):\n",
    "    frames = []\n",
    "    files =os.listdir(path) #采用listdir来读取所有文件\n",
    "    files.sort() #排序\n",
    "    for file_ in files:     #循环读取每个文件名\n",
    "        # if not os.path.isdir(path +file_):  #判断该文件是否是一个文件夹\n",
    "        f_name = str(file_)\n",
    "        frames.append(read_single_frame(path+f_name))\n",
    "    frames = np.stack((frames), axis=3)\n",
    "    return frames\n",
    "\n",
    "path = \"data/2yel/\"\n",
    "bpath = \"data/b2yel/\"\n",
    "testFrames = traverse_trajactory(path,input_shape)\n",
    "testFrames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 109, 104, 85)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic version of stacking input frames at first axis\n",
    "def traverse_trajactory(path, input_shape):\n",
    "    frames = []\n",
    "    files =os.listdir(path) #采用listdir来读取所有文件\n",
    "    files.sort() #排序\n",
    "    for file_ in files:     #循环读取每个文件名\n",
    "        # if not os.path.isdir(path +file_):  #判断该文件是否是一个文件夹\n",
    "        f_name = str(file_)\n",
    "        frames.append(read_single_frame(path+f_name))\n",
    "        # if frames.size == 0:\n",
    "        #     frames = read_single_frame(path+f_name)\n",
    "        # else:\n",
    "        #     frames = np.concatenate((frames,read_single_frame(path+f_name)),axis=-1)\n",
    "    frames = np.stack((frames), axis=0)\n",
    "    return frames\n",
    "\n",
    "path = \"data/2yel/\"\n",
    "testFrames = traverse_trajactory(path,input_shape)\n",
    "testFrames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109, 104, 850)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic version of concatenating input frames at last axis\n",
    "def traverse_trajactory(path, input_shape):\n",
    "    frames = []\n",
    "    files =os.listdir(path) #采用listdir来读取所有文件\n",
    "    files.sort() #排序\n",
    "    for file_ in files:     #循环读取每个文件名\n",
    "        # if not os.path.isdir(path +file_):  #判断该文件是否是一个文件夹\n",
    "        f_name = str(file_)\n",
    "        frames.append(read_single_frame(path+f_name))\n",
    "        # if frames.size == 0:\n",
    "        #     frames = read_single_frame(path+f_name)\n",
    "        # else:\n",
    "        #     frames = np.concatenate((frames,read_single_frame(path+f_name)),axis=-1)\n",
    "    frames = np.concatenate((frames), axis=-1)\n",
    "    return frames\n",
    "\n",
    "path = \"data/2yel/\"\n",
    "testFrames = traverse_trajactory(path,input_shape)\n",
    "testFrames.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "module_count = 1\n",
    "frame_count = 10\n",
    "depth = 109\n",
    "height = 104\n",
    "width = 85\n",
    "input_channels = 2\n",
    "input_shape =(module_count,depth,height,width,input_channels)\n",
    "\n",
    "# x_train = np.random.rand(100,depth,height,width,input_channels)\n",
    "# y_train = np.random.rand(100,1)\n",
    "\n",
    "x_test = np.random.rand(20,depth,height,width,input_channels)\n",
    "y_test = np.random.rand(20,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_frame(path,depth, height, width):\n",
    "    with open(path, \"rb\") as f:\n",
    "        npData = np.load(f)\n",
    "    frameData = npData.reshape((depth,height,width),order='F') # Using order F to maintain x-y-z order\n",
    "    return frameData\n",
    "\n",
    "def read_trajactory(path, field):\n",
    "    frames = []\n",
    "    files = os.listdir(path) #采用listdir来读取所有文件\n",
    "    files.sort() #排序\n",
    "    for file_ in files:     #循环读取每个文件名\n",
    "        f_name = str(file_)\n",
    "        if field == 1 and 'fld-01' in f_name:\n",
    "            frames.append(read_single_frame(path+f_name, depth, height, width))\n",
    "        elif field == 2 and 'fld-02' in f_name:\n",
    "            frames.append(read_single_frame(path+f_name, depth, height, width))\n",
    "        # else:\n",
    "        #     print('Invalid field number')\n",
    "    frames = np.stack((frames), axis=0)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv3D(filters=2, kernel_size=3, activation='relu', input_shape=(depth,height,width,input_channels)))\n",
    "    model.add(layers.MaxPooling3D(pool_size=2))\n",
    "    model.add(layers.Conv3D(filters=4, kernel_size=2, activation='relu'))\n",
    "    model.add(layers.MaxPooling3D(pool_size=2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(2, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "def train():\n",
    "    model = baseline_model()\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train, epochs=10, \n",
    "                        validation_data=(x_test, y_test))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 9s 901ms/sample - loss: 14087241728.0000 - accuracy: 0.0000e+00 - val_loss: 0.3825 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 6s 551ms/sample - loss: 0.6115 - accuracy: 0.0000e+00 - val_loss: 0.3833 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 5s 517ms/sample - loss: 0.6125 - accuracy: 0.0000e+00 - val_loss: 0.3839 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 6s 571ms/sample - loss: 0.6133 - accuracy: 0.0000e+00 - val_loss: 0.3843 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 6s 551ms/sample - loss: 0.6140 - accuracy: 0.0000e+00 - val_loss: 0.3847 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 5s 508ms/sample - loss: 0.6146 - accuracy: 0.0000e+00 - val_loss: 0.3851 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 5s 487ms/sample - loss: 0.6150 - accuracy: 0.0000e+00 - val_loss: 0.3854 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 5s 494ms/sample - loss: 0.6155 - accuracy: 0.0000e+00 - val_loss: 0.3856 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 5s 491ms/sample - loss: 0.6158 - accuracy: 0.0000e+00 - val_loss: 0.3859 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 5s 476ms/sample - loss: 0.6162 - accuracy: 0.0000e+00 - val_loss: 0.3861 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:From /Users/chenkaihu/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 16:30:05.334225: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/baseline/assets\n"
     ]
    }
   ],
   "source": [
    "path = 'data/b2yel/'\n",
    "stericMIF = read_trajactory(path, 1)\n",
    "elecMIF = read_trajactory(path, 2)\n",
    "x_train = np.stack((stericMIF, elecMIF), axis=4)\n",
    "y_train = np.ones((10,1))*0.7809668\n",
    "model = train()\n",
    "model.save('saved_model/baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "\n",
    "data = genfromtxt(\"data/WONKA_Equil_MD.csv\", delimiter=\",\")\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/WONKA_Equil_MD.csv\", sep=\",\", header=None)\n",
    "org_data = data[2:]\n",
    "\n",
    "module_list = org_data[:][0].to_list()\n",
    "pIC50 = org_data[:][3].to_list()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f6ac71a852bd6d80ce0d374c33eae97b00717625ad83596a01c3a0491541b1c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('fyp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
